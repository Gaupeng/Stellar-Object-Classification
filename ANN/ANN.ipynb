{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "colNames = ['class', 'ra'\n",
    "            , 'dec', 'u', 'g', 'r', 'i', 'z', 'nuv_mag']\n",
    "\n",
    "dataSet = pd.read_csv('../GALEX_data-extended-feats.csv', usecols = colNames)\n",
    "layers = [8, 32, 3]\n",
    "dataSet = pd.DataFrame(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(z):\n",
    "    return (1)/(1 + np.exp(-z))\n",
    "\n",
    "def sig_prime(z):\n",
    "    return sig(z)*(1-sig(z))\n",
    "\n",
    "def tanH(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanH_prime(z):\n",
    "    return 1 - ((tanH(z))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsHidden = np.array(np.random.randn(layers[1], layers[0]))*(np.sqrt(2/(layers[1] + layers[0])))\n",
    "biasHidden = np.zeros(layers[1])\n",
    "weightsOut = np.array(np.random.randn(layers[2], layers[1]))*(np.sqrt(2/(layers[1] + layers[2])))\n",
    "biasOut = np.zeros(layers[2])\n",
    "testDf = dataSet.sample(frac = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gaurang\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6573468173706127\n",
      "1 0.6573468173706127\n",
      "2 0.6573468173706127\n",
      "3 0.6686496133254015\n",
      "4 0.669244497323022\n",
      "5 0.6573468173706127\n",
      "6 0.5080309339678762\n",
      "7 0.60261748958953\n",
      "8 0.669244497323022\n",
      "9 0.6591314693634741\n",
      "10 0.6103509815585961\n",
      "11 0.6793575252825699\n",
      "12 0.5074360499702558\n",
      "13 0.5883402736466389\n",
      "14 0.6627007733491969\n",
      "15 0.4277215942891136\n",
      "16 0.6995835812016656\n",
      "17 0.5490779298036883\n",
      "18 0.6983938132064248\n",
      "19 0.5687091017251636\n",
      "20 0.702558001189768\n",
      "21 0.5431290898274836\n",
      "22 0.6591314693634741\n",
      "23 0.7150505651397977\n",
      "24 0.37775133848899467\n",
      "25 0.27483640690065436\n",
      "26 0.709101725163593\n",
      "27 0.5443188578227246\n",
      "28 0.7085068411659726\n",
      "29 0.17906008328375966\n",
      "30 0.7001784651992862\n",
      "31 0.31469363474122547\n",
      "32 0.7096966091612136\n",
      "33 0.6787626412849495\n",
      "34 0.5687091017251636\n",
      "35 0.4681737061273052\n",
      "36 0.6966091612135633\n",
      "37 0.635930993456276\n",
      "38 0.7120761451516954\n",
      "39 0.6811421772754312\n",
      "40 0.7108863771564545\n",
      "41 0.6650803093396788\n",
      "42 0.5395597858417609\n",
      "43 0.27900059488399764\n",
      "44 0.43723973825104107\n",
      "45 0.5800118976799524\n",
      "46 0.45568114217727546\n",
      "47 0.710291493158834\n",
      "48 0.570493753718025\n",
      "49 0.5907198096371208\n",
      "50 0.6508030933967877\n",
      "51 0.48483045806067815\n",
      "52 0.5764425936942297\n",
      "53 0.627007733491969\n",
      "54 0.5014872099940512\n",
      "55 0.6609161213563355\n",
      "56 0.6145151695419393\n",
      "57 0.49494348602022603\n",
      "58 0.6109458655562165\n",
      "59 0.6400951814396193\n",
      "60 0.5110053539559786\n",
      "61 0.644854253420583\n",
      "62 0.38905413444378345\n",
      "63 0.6698393813206425\n",
      "64 0.6960142772159429\n",
      "65 0.43367043426531826\n",
      "66 0.6751933372992267\n",
      "67 0.7120761451516954\n",
      "68 0.43902439024390244\n",
      "69 0.6787626412849495\n",
      "70 0.6157049375371803\n",
      "71 0.5841760856632957\n",
      "72 0.701368233194527\n",
      "73 0.46162998215348006\n",
      "74 0.43426531826293874\n",
      "75 0.4925639500297442\n",
      "76 0.6745984533016062\n",
      "77 0.5092207019631172\n",
      "78 0.546103509815586\n",
      "79 0.6418798334324807\n",
      "80 0.6597263533610946\n",
      "81 0.7132659131469363\n",
      "82 0.5288518738845925\n",
      "83 0.5389649018441404\n",
      "84 0.6252230814991077\n",
      "85 0.43902439024390244\n",
      "86 0.611540749553837\n",
      "87 0.4455681142177275\n",
      "88 0.5365853658536586\n",
      "89 0.3093396787626413\n",
      "90 0.5104104699583581\n",
      "91 0.49672813801308746\n",
      "92 0.6335514574657942\n",
      "93 0.5901249256395003\n",
      "94 0.5812016656751934\n",
      "95 0.6032123735871505\n",
      "96 0.5258774538964902\n",
      "97 0.5312314098750743\n",
      "98 0.6032123735871505\n",
      "99 0.5574063057703748\n"
     ]
    }
   ],
   "source": [
    "def expec(inp):\n",
    "    if inp == 0:\n",
    "        return np.array([1, 0, 0])\n",
    "    elif inp == 1:\n",
    "        return np.array([0, 1, 0])\n",
    "    else:\n",
    "        return np.array([0, 0, 1])\n",
    "\n",
    "def feedFor(trainDf, row, weightsHidden, biasHidden, weightsOut, biasOut):\n",
    "    x = np.array(trainDf.iloc[row, 1:])\n",
    "    tempDot = np.array(np.dot(weightsHidden, x.T)) # 20 x 1\n",
    "    hiddenLI = np.add(tempDot, biasHidden) # 1 x 20\n",
    "    hiddenAct = tanH(hiddenLI) # 1 x 20\n",
    "    tempDot2 = np.dot(weightsOut, hiddenAct)\n",
    "    outputLI = np.add(tempDot2, biasOut.T)\n",
    "    output = tanH(outputLI)\n",
    "#     print(output)\n",
    "    return x, output, hiddenAct\n",
    "\n",
    "def backProp(trainDf, layers, x, row, output, hiddenAct, weightsHidden, biasHidden, weightsOut, biasOut, eta):\n",
    "    actClass = int(trainDf.iloc[row, 0])\n",
    "    expected = expec(actClass)\n",
    "    error = expected - output\n",
    "    slopeOutput = tanH_prime(np.array(output))\n",
    "    slopeHL = tanH_prime(np.array(hiddenAct))\n",
    "    deltaO = error*slopeOutput\n",
    "    errHL = np.dot(weightsOut.T, deltaO)\n",
    "    deltaHL = errHL*slopeHL\n",
    "    \n",
    "    deltaO = np.reshape(deltaO, (3, 1))\n",
    "    hiddenAct = np.reshape(hiddenAct, (layers[1], 1))\n",
    "    tempDot = np.dot(deltaO, hiddenAct.T)*(eta) # 3 x 20\n",
    "    weightsOut += tempDot\n",
    "    \n",
    "    deltaHL = np.reshape(deltaHL, (layers[1], 1))\n",
    "    x = np.reshape(x, (8, 1))\n",
    "    tempDot2 = np.dot(deltaHL, x.T)*(eta)\n",
    "    weightsHidden += tempDot2\n",
    "    biasHidden += np.sum(deltaHL, axis = 0)*(eta)\n",
    "    biasOut += np.sum(deltaO, axis = 0)*(eta)\n",
    "\n",
    "\n",
    "def runNN(layers, trainDf, weightsHidden, biasHidden, weightsOut, biasOut, eta):\n",
    "    for row in range(len(trainDf)):\n",
    "        x, output, hiddenAct = feedFor(trainDf, row, weightsHidden, biasHidden, weightsOut, biasOut)\n",
    "        backProp(trainDf, layers, x, row, output, hiddenAct, weightsHidden, biasHidden, weightsOut, biasOut, eta)\n",
    "\n",
    "def accu():\n",
    "    correct = 0\n",
    "    for row in range(len(testDf)):\n",
    "        actClass = int(testDf.iloc[row, 0])\n",
    "        x = np.array(testDf.iloc[row, 1:])\n",
    "        tempDot = np.array(np.dot(weightsHidden, x.T)) # 20 x 1\n",
    "        hiddenLI = np.add(tempDot, biasHidden) # 1 x 20\n",
    "        hiddenAct = tanH(hiddenLI) # 1 x 20\n",
    "        tempDot2 = np.dot(weightsOut, hiddenAct)\n",
    "        outputLI = np.add(tempDot2, biasOut.T)\n",
    "        output = tanH(outputLI)\n",
    "        expected = expec(actClass)\n",
    "        if np.argmax(expected) == np.argmax(output):\n",
    "            #print(\"Instance classified as: \", np.argmax(output))\n",
    "            correct += 1\n",
    "        else:\n",
    "            pass\n",
    "            #print(\"Instance misclassified as: \", np.argmax(output))\n",
    "        #print(output)\n",
    "    return correct/len(testDf)\n",
    "\n",
    "filename = \"accuracy_0.01_32.csv\"\n",
    "fields = ['Training Data', 'Accuracy']\n",
    "with open(filename, 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(fields) \n",
    "    eta = 0.01\n",
    "    lenDf = 0\n",
    "    for i in range(100):\n",
    "        trainDf = dataSet.sample(frac = 0.8)\n",
    "        lenDf += len(trainDf)\n",
    "        runNN(layers, trainDf, weightsHidden, biasHidden, weightsOut, biasOut, eta)\n",
    "        acc = accu()\n",
    "        newRow = [lenDf, acc]\n",
    "        csvwriter.writerow(newRow)\n",
    "        print(i, acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"wH\", weightsHidden)\n",
    "np.save(\"bH\", biasHidden)\n",
    "np.save(\"wO\", weightsOut)\n",
    "np.save(\"bO\", biasOut)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

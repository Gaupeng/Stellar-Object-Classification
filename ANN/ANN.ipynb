{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "colNames = ['class', 'ra'\n",
    "            , 'dec', 'u', 'g', 'r', 'i', 'z', 'nuv_mag']\n",
    "\n",
    "dataSet = pd.read_csv('../GALEX_data-extended-feats.csv', usecols = colNames)\n",
    "layers = [8, 32, 3]\n",
    "dataSet = pd.DataFrame(dataSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(z):\n",
    "    return (1)/(1 + np.exp(-z))\n",
    "\n",
    "def sig_prime(z):\n",
    "    return sig(z)*(1-sig(z))\n",
    "\n",
    "def tanH(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanH_prime(z):\n",
    "    return 1 - ((tanH(z))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsHidden = np.array(np.random.randn(layers[1], layers[0]))*(np.sqrt(2/(layers[1] + layers[0])))\n",
    "biasHidden = np.zeros(layers[1])\n",
    "weightsOut = np.array(np.random.randn(layers[2], layers[1]))*(np.sqrt(2/(layers[1] + layers[2])))\n",
    "biasOut = np.zeros(layers[2])\n",
    "testDf = dataSet.sample(frac = 0.2, random_state = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  62.0817843866171%\n",
      "Precision for Class 0:  12.658227848101266%\n",
      "Recall for Class 0:  2.797202797202797%\n",
      "Precision for Class 1:  66.32839793711528%\n",
      "Recall for Class 1:  90.42866863234293%\n",
      "Precision for Class 2:  30.215827338129497%\n",
      "Recall for Class 2:  10.493441599000624%\n",
      "\n",
      "Accuracy is:  62.95910780669145%\n",
      "Precision for Class 0:  13.013698630136986%\n",
      "Recall for Class 0:  2.606310013717421%\n",
      "Precision for Class 1:  67.84186205716242%\n",
      "Recall for Class 1:  89.56168097605061%\n",
      "Precision for Class 2:  34.10326086956522%\n",
      "Recall for Class 2:  15.987261146496815%\n",
      "\n",
      "Accuracy is:  64.37174721189591%\n",
      "Precision for Class 0:  18.823529411764707%\n",
      "Recall for Class 0:  4.401650618982118%\n",
      "Precision for Class 1:  70.17140837603817%\n",
      "Recall for Class 1:  89.43693693693695%\n",
      "Precision for Class 2:  36.38392857142857%\n",
      "Recall for Class 2:  20.92426187419769%\n",
      "\n",
      "Accuracy is:  65.90334572490706%\n",
      "Precision for Class 0:  22.89156626506024%\n",
      "Recall for Class 0:  5.420827389443652%\n",
      "Precision for Class 1:  71.78786221979223%\n",
      "Recall for Class 1:  88.73620184726289%\n",
      "Precision for Class 2:  42.44402985074627%\n",
      "Recall for Class 2:  28.706624605678233%\n",
      "\n",
      "Accuracy is:  67.0185873605948%\n",
      "Precision for Class 0:  23.232323232323232%\n",
      "Recall for Class 0:  6.406685236768802%\n",
      "Precision for Class 1:  73.81756756756756%\n",
      "Recall for Class 1:  88.8211382113821%\n",
      "Precision for Class 2:  44.03669724770643%\n",
      "Recall for Class 2:  33.43888537048765%\n",
      "\n",
      "Accuracy is:  68.63940520446097%\n",
      "Precision for Class 0:  24.550898203592812%\n",
      "Recall for Class 0:  5.774647887323944%\n",
      "Precision for Class 1:  75.33496886204945%\n",
      "Recall for Class 1:  89.52679973088136%\n",
      "Precision for Class 2:  46.306592533756955%\n",
      "Recall for Class 2:  37.46786632390746%\n",
      "\n",
      "Accuracy is:  69.71003717472118%\n",
      "Precision for Class 0:  23.863636363636363%\n",
      "Recall for Class 0:  5.9071729957805905%\n",
      "Precision for Class 1:  76.84634567427585%\n",
      "Recall for Class 1:  90.26588553402433%\n",
      "Precision for Class 2:  47.90419161676647%\n",
      "Recall for Class 2:  40.609137055837564%\n",
      "\n",
      "Accuracy is:  71.44981412639405%\n",
      "Precision for Class 0:  28.57142857142857%\n",
      "Recall for Class 0:  5.698005698005698%\n",
      "Precision for Class 1:  77.74331136099264%\n",
      "Recall for Class 1:  90.68294889190412%\n",
      "Precision for Class 2:  52.90819901892081%\n",
      "Recall for Class 2:  47.158026233604%\n",
      "\n",
      "Accuracy is:  72.80297397769516%\n",
      "Precision for Class 0:  18.796992481203006%\n",
      "Recall for Class 0:  3.5919540229885056%\n",
      "Precision for Class 1:  79.49367088607595%\n",
      "Recall for Class 1:  91.83352080989876%\n",
      "Precision for Class 2:  54.152367879203844%\n",
      "Recall for Class 2:  49.81060606060606%\n",
      "\n",
      "Accuracy is:  73.07063197026022%\n",
      "Precision for Class 0:  24.550898203592812%\n",
      "Recall for Class 0:  5.647382920110193%\n",
      "Precision for Class 1:  79.84267453294002%\n",
      "Recall for Class 1:  92.1261629226231%\n",
      "Precision for Class 2:  55.193482688391036%\n",
      "Recall for Class 2:  51.0678391959799%\n",
      "\n",
      "Accuracy is:  74.40892193308551%\n",
      "Precision for Class 0:  27.64227642276423%\n",
      "Recall for Class 0:  4.781997187060478%\n",
      "Precision for Class 1:  80.8543993729179%\n",
      "Recall for Class 1:  93.05367613892648%\n",
      "Precision for Class 2:  56.30420280186791%\n",
      "Recall for Class 2:  53.417721518987335%\n",
      "\n",
      "Accuracy is:  74.8996282527881%\n",
      "Precision for Class 0:  27.77777777777778%\n",
      "Recall for Class 0:  6.983240223463687%\n",
      "Precision for Class 1:  81.67518678725915%\n",
      "Recall for Class 1:  93.05555555555556%\n",
      "Precision for Class 2:  57.0938999314599%\n",
      "Recall for Class 2:  53.915857605178%\n",
      "\n",
      "Accuracy is:  75.0185873605948%\n",
      "Precision for Class 0:  27.142857142857142%\n",
      "Recall for Class 0:  5.241379310344827%\n",
      "Precision for Class 1:  81.70756034310791%\n",
      "Recall for Class 1:  92.8377153218495%\n",
      "Precision for Class 2:  57.95165394402035%\n",
      "Recall for Class 2:  57.367758186397985%\n",
      "\n",
      "Accuracy is:  74.91449814126395%\n",
      "Precision for Class 0:  27.37430167597765%\n",
      "Recall for Class 0:  6.77731673582296%\n",
      "Precision for Class 1:  82.04458726651939%\n",
      "Recall for Class 1:  92.7355278093076%\n",
      "Precision for Class 2:  57.68985322271857%\n",
      "Recall for Class 2:  56.606136505948655%\n",
      "\n",
      "Accuracy is:  76.08921933085502%\n",
      "Precision for Class 0:  27.27272727272727%\n",
      "Recall for Class 0:  6.11353711790393%\n",
      "Precision for Class 1:  82.70962577546528%\n",
      "Recall for Class 1:  93.44336423242143%\n",
      "Precision for Class 2:  59.84752223634053%\n",
      "Recall for Class 2:  58.328173374613%\n",
      "\n",
      "Accuracy is:  76.4014869888476%\n",
      "Precision for Class 0:  30.303030303030305%\n",
      "Recall for Class 0:  6.738544474393531%\n",
      "Precision for Class 1:  83.41678349689565%\n",
      "Recall for Class 1:  94.25209323376329%\n",
      "Precision for Class 2:  58.90236119974473%\n",
      "Recall for Class 2:  59.0153452685422%\n",
      "\n",
      "Accuracy is:  77.2639405204461%\n",
      "Precision for Class 0:  30.555555555555557%\n",
      "Recall for Class 0:  6.0606060606060606%\n",
      "Precision for Class 1:  83.90665861999598%\n",
      "Recall for Class 1:  94.15349887133183%\n",
      "Precision for Class 2:  60.93167701863354%\n",
      "Recall for Class 2:  62.523900573613766%\n",
      "\n",
      "Accuracy is:  77.1449814126394%\n",
      "Precision for Class 0:  18.439716312056735%\n",
      "Recall for Class 0:  3.6568213783403656%\n",
      "Precision for Class 1:  84.56389452332658%\n",
      "Recall for Class 1:  93.95988280369619%\n",
      "Precision for Class 2:  60.03627569528416%\n",
      "Recall for Class 2:  62.96766011414078%\n",
      "\n",
      "Accuracy is:  77.32342007434944%\n",
      "Precision for Class 0:  24.390243902439025%\n",
      "Recall for Class 0:  4.304160688665711%\n",
      "Precision for Class 1:  84.33319821645723%\n",
      "Recall for Class 1:  94.07641872032558%\n",
      "Precision for Class 2:  60.4916067146283%\n",
      "Recall for Class 2:  62.86604361370717%\n",
      "\n",
      "Accuracy is:  77.41263940520446%\n",
      "Precision for Class 0:  27.167630057803464%\n",
      "Recall for Class 0:  6.666666666666667%\n",
      "Precision for Class 1:  84.97074843655437%\n",
      "Recall for Class 1:  94.31258396775638%\n",
      "Precision for Class 2:  59.373040752351095%\n",
      "Recall for Class 2:  60.939510939510946%\n",
      "\n",
      "Accuracy is:  77.4275092936803%\n",
      "Precision for Class 0:  21.138211382113823%\n",
      "Recall for Class 0:  3.494623655913978%\n",
      "Precision for Class 1:  84.67048710601719%\n",
      "Recall for Class 1:  94.25837320574163%\n",
      "Precision for Class 2:  60.83916083916085%\n",
      "Recall for Class 2:  65.57788944723619%\n",
      "\n",
      "Accuracy is:  77.56133828996282%\n",
      "Precision for Class 0:  26.490066225165563%\n",
      "Recall for Class 0:  5.398110661268556%\n",
      "Precision for Class 1:  84.83383685800604%\n",
      "Recall for Class 1:  94.88623563865735%\n",
      "Precision for Class 2:  59.91298943443132%\n",
      "Recall for Class 2:  62.394822006472495%\n",
      "\n",
      "Accuracy is:  77.54646840148699%\n",
      "Precision for Class 0:  28.947368421052634%\n",
      "Recall for Class 0:  6.094182825484765%\n",
      "Precision for Class 1:  84.69779664443097%\n",
      "Recall for Class 1:  94.26321709786278%\n",
      "Precision for Class 2:  60.332103321033216%\n",
      "Recall for Class 2:  62.965340179717586%\n",
      "\n",
      "Accuracy is:  78.11152416356877%\n",
      "Precision for Class 0:  23.200000000000003%\n",
      "Recall for Class 0:  4.084507042253521%\n",
      "Precision for Class 1:  85.45602275035547%\n",
      "Recall for Class 1:  94.68827368894891%\n",
      "Precision for Class 2:  60.64400715563506%\n",
      "Recall for Class 2:  64.69465648854961%\n",
      "\n",
      "Accuracy is:  78.15613382899627%\n",
      "Precision for Class 0:  29.605263157894733%\n",
      "Recall for Class 0:  6.446991404011461%\n",
      "Precision for Class 1:  85.20024395202277%\n",
      "Recall for Class 1:  94.2433100966944%\n",
      "Precision for Class 2:  61.66868198307134%\n",
      "Recall for Class 2:  64.55696202531645%\n",
      "\n",
      "Accuracy is:  78.34944237918215%\n",
      "Precision for Class 0:  32.666666666666664%\n",
      "Recall for Class 0:  6.872370266479663%\n",
      "Precision for Class 1:  85.28814935064936%\n",
      "Recall for Class 1:  94.8116399729303%\n",
      "Precision for Class 2:  61.74863387978142%\n",
      "Recall for Class 2:  64.40785307156428%\n",
      "\n",
      "Accuracy is:  79.19702602230483%\n",
      "Precision for Class 0:  27.27272727272727%\n",
      "Recall for Class 0:  5.042016806722689%\n",
      "Precision for Class 1:  86.67901997117562%\n",
      "Recall for Class 1:  94.96954658244981%\n",
      "Precision for Class 2:  62.21198156682027%\n",
      "Recall for Class 2:  68.44106463878326%\n",
      "\n",
      "Accuracy is:  78.51301115241635%\n",
      "Precision for Class 0:  26.056338028169012%\n",
      "Recall for Class 0:  5.24822695035461%\n",
      "Precision for Class 1:  85.76679599754952%\n",
      "Recall for Class 1:  94.7653429602888%\n",
      "Precision for Class 2:  61.86239620403321%\n",
      "Recall for Class 2:  65.6801007556675%\n",
      "\n",
      "Accuracy is:  79.06319702602231%\n",
      "Precision for Class 0:  21.296296296296298%\n",
      "Recall for Class 0:  3.203342618384401%\n",
      "Precision for Class 1:  86.24794745484401%\n",
      "Recall for Class 1:  95.08938673908123%\n",
      "Precision for Class 2:  62.57879656160459%\n",
      "Recall for Class 2:  68.76574307304786%\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  79.8364312267658%\n",
      "Precision for Class 0:  27.11864406779661%\n",
      "Recall for Class 0:  4.539007092198582%\n",
      "Precision for Class 1:  86.76651620845301%\n",
      "Recall for Class 1:  95.20486267447096%\n",
      "Precision for Class 2:  63.93537218695903%\n",
      "Recall for Class 2:  70.21546261089988%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def expec(inp):\n",
    "    if inp == 0:\n",
    "        return np.array([1, 0, 0])\n",
    "    elif inp == 1:\n",
    "        return np.array([0, 1, 0])\n",
    "    else:\n",
    "        return np.array([0, 0, 1])\n",
    "    \n",
    "\n",
    "def feedFor(trainDf, row, weightsHidden, biasHidden, weightsOut, biasOut):\n",
    "    x = np.array(trainDf.iloc[row, 1:])\n",
    "    tempDot = np.array(np.dot(weightsHidden, x.T)) \n",
    "    hiddenLI = np.add(tempDot, biasHidden)\n",
    "    hiddenAct = tanH(hiddenLI) \n",
    "    tempDot2 = np.dot(weightsOut, hiddenAct)\n",
    "    outputLI = np.add(tempDot2, biasOut.T)\n",
    "    output = tanH(outputLI)\n",
    "    return x, output, hiddenAct\n",
    "\n",
    "def backProp(trainDf, layers, x, row, output, hiddenAct, weightsHidden, biasHidden, weightsOut, biasOut, eta):\n",
    "    actClass = int(trainDf.iloc[row, 0])\n",
    "    expected = expec(actClass)\n",
    "    error = expected - output\n",
    "    slopeOutput = tanH_prime(np.array(output))\n",
    "    slopeHL = tanH_prime(np.array(hiddenAct))\n",
    "    deltaO = error*slopeOutput\n",
    "    errHL = np.dot(weightsOut.T, deltaO)\n",
    "    deltaHL = errHL*slopeHL\n",
    "    \n",
    "    deltaO = np.reshape(deltaO, (3, 1))\n",
    "    hiddenAct = np.reshape(hiddenAct, (layers[1], 1))\n",
    "    tempDot = np.dot(deltaO, hiddenAct.T)*(eta)\n",
    "    weightsOut += tempDot\n",
    "    \n",
    "    deltaHL = np.reshape(deltaHL, (layers[1], 1))\n",
    "    x = np.reshape(x, (8, 1))\n",
    "    tempDot2 = np.dot(deltaHL, x.T)*(eta)\n",
    "    weightsHidden += tempDot2\n",
    "    biasHidden += np.sum(deltaHL, axis = 0)*(eta)\n",
    "    biasOut += np.sum(deltaO, axis = 0)*(eta)\n",
    "    \n",
    "    return (expected, output)\n",
    "\n",
    "\n",
    "def runNN(layers, trainDf, weightsHidden, biasHidden, weightsOut, biasOut, eta, confMatrix):\n",
    "    confMatrix = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
    "    for row in range(len(trainDf)):\n",
    "        x, output, hiddenAct = feedFor(trainDf, row, weightsHidden, biasHidden, weightsOut, biasOut)\n",
    "        exp, out = backProp(trainDf, layers, x, row, output, hiddenAct, weightsHidden, biasHidden, weightsOut, biasOut, eta)\n",
    "        \n",
    "        if exp[0] == 1:\n",
    "            if max(out) == out[0]:\n",
    "                confMatrix[0][0] += 1\n",
    "            elif max(out) == out[1]:\n",
    "                confMatrix[1][0] += 1\n",
    "            else:\n",
    "                confMatrix[2][0] += 1\n",
    "                \n",
    "        if exp[1] == 1:\n",
    "            if max(out) == out[0]:\n",
    "                confMatrix[0][1] += 1\n",
    "            elif max(out) == out[1]:\n",
    "                confMatrix[1][1] += 1\n",
    "            else:\n",
    "                confMatrix[2][1] += 1\n",
    "        \n",
    "        if exp[2] == 1:\n",
    "            if max(out) == out[0]:\n",
    "                confMatrix[0][2] += 1\n",
    "            elif max(out) == out[1]:\n",
    "                confMatrix[1][2] += 1\n",
    "            else:\n",
    "                confMatrix[2][2] += 1\n",
    "                \n",
    "    \n",
    "    acc = (confMatrix[0][0] + confMatrix[1][1] + confMatrix[2][2])/len(trainDf)\n",
    "    prec0 = confMatrix[0][0]/(confMatrix[0][0] + confMatrix[0][1] + confMatrix[0][2])\n",
    "    prec1 = confMatrix[1][1]/(confMatrix[1][1] + confMatrix[1][0] + confMatrix[1][2])\n",
    "    prec2 = confMatrix[2][2]/(confMatrix[2][2] + confMatrix[2][0] + confMatrix[2][1])\n",
    "    rec0 = confMatrix[0][0]/(confMatrix[0][0] + confMatrix[1][0] + confMatrix[2][0])\n",
    "    rec1 = confMatrix[1][1]/(confMatrix[1][1] + confMatrix[0][1] + confMatrix[2][1])\n",
    "    rec2 = confMatrix[2][2]/(confMatrix[2][2] + confMatrix[0][2] + confMatrix[1][2])\n",
    "    \n",
    "    print(\"Accuracy is: \", str(acc*100) + \"%\")\n",
    "    print(\"Precision for Class 0: \", str(prec0*100) + \"%\")\n",
    "    print(\"Recall for Class 0: \", str(rec0*100) + \"%\")\n",
    "    print(\"Precision for Class 1: \", str(prec1*100) + \"%\")\n",
    "    print(\"Recall for Class 1: \", str(rec1*100) + \"%\")\n",
    "    print(\"Precision for Class 2: \", str(prec2*100) + \"%\")\n",
    "    print(\"Recall for Class 2: \", str(rec2*100) + \"%\")\n",
    "    \n",
    "    return acc\n",
    "    \n",
    "    \n",
    "filename = \"WriteToMe.csv\"\n",
    "fields = ['Training Data', 'Accuracy']\n",
    "                         \n",
    "with open(filename, 'w') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(fields)\n",
    "                         \n",
    "    eta = 0.01\n",
    "    lenDf = 0\n",
    "                         \n",
    "    for i in range(30):\n",
    "        confMatrix = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
    "        trainDf = dataSet.sample(frac = 0.8)\n",
    "        lenDf += len(trainDf)\n",
    "        \n",
    "        acc = runNN(layers, trainDf, weightsHidden, biasHidden, weightsOut, biasOut, eta, confMatrix)\n",
    "        \n",
    "        \n",
    "        newRow = [lenDf, acc]\n",
    "        csvwriter.writerow(newRow)\n",
    "        print()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "np.save(\"wH\", weightsHidden)\n",
    "np.save(\"bH\", biasHidden)\n",
    "np.save(\"wO\", weightsOut)\n",
    "np.save(\"bO\", biasOut)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
